{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "g_env = UnityEnvironment(file_name='./Reacher_Linux_Multi/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_action_size:4\n",
      "g_state_size:33\n"
     ]
    }
   ],
   "source": [
    "g_brain_name = g_env.brain_names[0]\n",
    "g_brain = g_env.brains[g_brain_name]\n",
    "g_env_info = g_env.reset(train_mode=True)[g_brain_name]\n",
    "g_num_agents = len(g_env_info.agents)\n",
    "g_action_size = g_brain.vector_action_space_size\n",
    "g_state_size = g_env_info.vector_observations.shape[1]\n",
    "print(\"g_action_size:{}\".format(g_action_size))\n",
    "print(\"g_state_size:{}\".format(g_state_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "g_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "g_dist_std = torch.Tensor(nn.Parameter(torch.zeros(1, g_action_size)))\n",
    "\n",
    "def states_to_prob(policy, states):\n",
    "    states = torch.stack(states)\n",
    "    #policy_input = states.view(-1,*states.shape[-3:])\n",
    "    #return policy(policy_input).view(states.shape[:-3])\n",
    "    return policy(states)\n",
    "\n",
    "def clipped_surrogate(policy, old_probs, \n",
    "                      states, actions, \n",
    "                      rewards, values, \n",
    "                      gea, target_value,\n",
    "                      discount=0.995,\n",
    "                      epsilon=0.1, beta=0.01):\n",
    "\n",
    "    advantages_mean = np.mean(gea)\n",
    "    advantages_std = np.std(gea) + 1.0e-10\n",
    "    advantages_normalized = (gea - advantages_mean)/advantages_std\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    actions = torch.tensor(actions, dtype=torch.float, device=g_device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=g_device)\n",
    "    advantages = torch.tensor(advantages_normalized, dtype=torch.float, device=g_device)\n",
    "    values = torch.tensor(np.array(values), dtype=torch.float, device=g_device)\n",
    "    target_value = torch.tensor(target_value, dtype=torch.float, device=g_device, requires_grad=False)\n",
    "    \n",
    "    states = torch.stack(states)\n",
    "    est_actions, est_values = policy(states)\n",
    "    dists = torch.distributions.Normal(est_actions, F.softplus(g_dist_std.to(g_device)))\n",
    "    #dists = torch.distributions.Normal(est_actions, g_dist_std.to(g_device))\n",
    "    #actions = dists.sample()\n",
    "    #actions.clamp_(min=-1.0, max=1.0)\n",
    "        \n",
    "    log_prob = dists.log_prob(actions)\n",
    "    log_prob = torch.sum(log_prob, dim=-1, keepdim=True)\n",
    "    \n",
    "    new_probs = log_prob\n",
    "        \n",
    "    # entropy_loss = torch.Tensor(np.zeros((log_prob.size(0), 1)))\n",
    "    entropy_loss = dists.entropy()\n",
    "    entropy_loss = torch.sum(entropy_loss, dim=-1, keepdim=True)/4.0\n",
    "    \n",
    "    # ratio for clipping\n",
    "    ratio = torch.exp(new_probs-old_probs).squeeze(-1)\n",
    "\n",
    "    # clipped function\n",
    "    clip = torch.clamp(ratio, 1.0-epsilon, 1.0+epsilon)\n",
    "    clipped_sur = torch.min(ratio*advantages,clip*advantages)\n",
    "    \n",
    "    critic_loss = F.smooth_l1_loss(est_values.squeeze(),target_value.squeeze())\n",
    "    #critic_loss = 0.5 * (est_values.squeeze() - target_value.squeeze()).pow(2).mean()\n",
    "    return torch.mean(clipped_sur + beta*entropy_loss.squeeze(-1)), critic_loss, entropy_loss, clipped_sur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def collect_trajectories(envs, policy, tmax=200, nrand=5, train_mode=False):\n",
    "\n",
    "    #initialize returning lists and start the game!\n",
    "    state_list=[]\n",
    "    reward_list=[]\n",
    "    prob_list=[]\n",
    "    action_list=[]\n",
    "    value_list=[]\n",
    "\n",
    "    env_info = envs.reset(train_mode=train_mode)[g_brain_name]\n",
    "    #env_info = envs.reset(train_mode=train_mode,config={'goal_speed':0.0,'goal_size':10.0})[g_brain_name]\n",
    "\n",
    "    # perform nrand random steps\n",
    "    for _ in range(nrand):\n",
    "        action = np.random.randn(g_num_agents, g_action_size)\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "        env_info = envs.step(action)[g_brain_name]\n",
    "\n",
    "    for t in range(tmax):\n",
    "        #state = torch.from_numpy(env_info.vector_observations).float().unsqueeze(0).to(g_device)\n",
    "        state = torch.from_numpy(env_info.vector_observations).float().to(g_device)\n",
    "        est_action, value = policy(state)\n",
    "        est_action = est_action.squeeze().cpu().detach()\n",
    "        dist = torch.distributions.Normal(est_action, F.softplus(g_dist_std))\n",
    "        action = dist.sample().numpy()\n",
    "        action = np.clip(action,-1.0,1.0)\n",
    "        value = value.squeeze().cpu().detach().numpy()\n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "        log_prob = torch.sum(log_prob, dim=1, keepdim=True).cpu().detach().numpy()\n",
    "        env_info = envs.step(action)[g_brain_name]\n",
    "\n",
    "        reward = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "\n",
    "        state_list.append(state)\n",
    "        reward_list.append(reward)\n",
    "        prob_list.append(log_prob)\n",
    "        action_list.append(action)\n",
    "        value_list.append(value)\n",
    "\n",
    "        # stop if any of the trajectories is done to have retangular lists\n",
    "        if np.any(dones):\n",
    "            break\n",
    "    gea_list = []\n",
    "    target_value_list = []\n",
    "    for n in range(g_num_agents):\n",
    "        gea = [0.0] * len(state_list)\n",
    "        target_value = [0.0] * len(state_list)\n",
    "        i_max = len(state_list)\n",
    "        done = 0\n",
    "        TAU = 0.95\n",
    "        discount = 0.99\n",
    "        returns_ = 0.0\n",
    "        advantages_ = 0.0\n",
    "        done = 1.0\n",
    "        for i in reversed(range(i_max)):\n",
    "            rwrds_ = reward_list[i][n]\n",
    "            values_ = value_list[i][n]\n",
    "            next_value_ = value_list[min(i_max-1, i + 1)][n]\n",
    "            td_error = rwrds_ + (discount * next_value_*(1.0-done)) - values_\n",
    "            advantages_ = (advantages_ * TAU * discount*(1.0-done)) + td_error\n",
    "            gea[i] = advantages_\n",
    "            returns_ = (discount*returns_*(1.0-done)) + rwrds_\n",
    "            target_value[i] = returns_\n",
    "            done = 0.0\n",
    "\n",
    "        gea = np.cumsum(gea)\n",
    "        gea_list.append(deepcopy(gea))\n",
    "        target_value_list.append(deepcopy(target_value))\n",
    "\n",
    "    gea_list= list(map(list, zip(*gea_list)))\n",
    "    target_value_list= list(map(list, zip(*target_value_list)))\n",
    "    \n",
    "    \n",
    "    # return states, actions, rewards\n",
    "    return prob_list, state_list, action_list, reward_list, value_list, gea_list, target_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from policy import Policy\n",
    "\n",
    "# run your own policy!\n",
    "policy=Policy(state_size=g_state_size,\n",
    "              action_size=g_action_size,\n",
    "              hidden_layers=[64, 64],\n",
    "              seed=0).to(g_device)\n",
    "\n",
    "# we use the adam optimizer with learning rate 2e-4\n",
    "# optim.SGD is also possible\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 20, average score: 5.20850065\n",
      "Episode: 40, average score: 4.89500045\n",
      "Episode: 60, average score: 4.92183317\n",
      "Episode: 80, average score: 4.82825015\n",
      "Episode: 100, average score: 4.8263006\n",
      "Episode: 120, average score: 4.85400037\n",
      "Episode: 140, average score: 4.96750075\n",
      "Episode: 160, average score: 5.00880095\n",
      "Episode: 180, average score: 4.95280034\n",
      "Episode: 200, average score: 4.80240073\n",
      "Average Score: 4.80\n",
      "Elapsed time: 0:03:46.529063\n",
      "Saving checkpoint!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from collections import deque\n",
    "import timeit\n",
    "from datetime import timedelta\n",
    "\n",
    "scores_window = deque(maxlen=100)  # last 100 scores\n",
    "\n",
    "discount = 0.99\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "SGD_epoch = 4\n",
    "episode = 50\n",
    "batch_size = 64\n",
    "#tmax = max(10*batch_size,int(30.0/0.1),1024)\n",
    "tmax = batch_size*3\n",
    "\n",
    "print_per_n = min(50,episode/10)\n",
    "counter = 0\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for e in range(episode):\n",
    "    old_probs, states, actions, rewards, values, gea, target_value = collect_trajectories(envs=g_env, \n",
    "                                                                                          policy=policy, \n",
    "                                                                                          tmax=tmax,\n",
    "                                                                                          nrand = 5,\n",
    "                                                                                          train_mode=True)\n",
    "\n",
    "    total_rewards = np.sum(rewards)\n",
    "    scores_window.append(total_rewards)\n",
    "\n",
    "    # gradient ascent step\n",
    "    n_sample = len(old_probs)//batch_size\n",
    "    idx = np.arange(len(old_probs))\n",
    "    np.random.shuffle(idx)\n",
    "    for b in range(n_sample):\n",
    "        #ind = np.random.randint(len(old_probs),size=batch_size)\n",
    "        #ind = np.random.randint(len(old_probs)-batch_size-1,size=1)\n",
    "        #ind = range(b*batch_size,(b+1)*batch_size)\n",
    "        ind = idx[b*batch_size:(b+1)*batch_size]\n",
    "        #ind = ind[0]\n",
    "        #ind = 0\n",
    "        op = [old_probs[i] for i in ind]\n",
    "        s = [states[i] for i in ind]\n",
    "        a = [actions[i] for i in ind]\n",
    "        r = [rewards[i] for i in ind]\n",
    "        v = [values[i] for i in ind]\n",
    "        g = [gea[i] for i in ind]\n",
    "        tv = [target_value[i] for i in ind]\n",
    "        for epoch in range(SGD_epoch):\n",
    "            l_clip, critic_loss, entropy_loss, clipped_sur = clipped_surrogate(policy=policy,\n",
    "                                                                               old_probs=op,\n",
    "                                                                               states=s,\n",
    "                                                                               actions=a,\n",
    "                                                                               rewards=r,\n",
    "                                                                               values=v,\n",
    "                                                                               gea = g,\n",
    "                                                                               target_value = tv,\n",
    "                                                                               discount = discount,\n",
    "                                                                               epsilon=epsilon,\n",
    "                                                                               beta=beta)\n",
    "            L = -l_clip+critic_loss\n",
    "            #print(\"-l_clip:{}\".format(-l_clip), end=\"\\n\")\n",
    "            #print(\"critic_loss:{}\".format(critic_loss), end=\"\\n\")\n",
    "            #print(\"clipped_sur:{}\".format(torch.mean(clipped_sur)), end=\"\\n\")\n",
    "            #print(\"L:{}\".format(L))\n",
    "            optimizer.zero_grad()\n",
    "            # we need to specify retain_graph=True on the backward pass\n",
    "            # this is because pytorch automatically frees the computational graph after\n",
    "            # the backward pass to save memory\n",
    "            # Without the computational graph, the chain of derivative is lost\n",
    "            L.backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(policy.parameters(), 100.0)\n",
    "            optimizer.step()\n",
    "            del L\n",
    "\n",
    "    # the clipping parameter reduces as time goes on\n",
    "    epsilon*=.999\n",
    "    \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # display some progress every 25 iterations\n",
    "    if (e+1)%print_per_n ==0 :\n",
    "        print(\"Episode: {0:d}, average score: {1:f}\".format(e+1,np.mean(scores_window)), end=\"\\n\")\n",
    "    else:\n",
    "        print(\"Episode: {0:d}, score: {1}\".format(e+1, total_rewards), end=\"\\r\")\n",
    "    if np.mean(scores_window)<5.0:\n",
    "        counter = 0\n",
    "    if e>=100 and np.mean(scores_window)>30.0:\n",
    "        counter += 1\n",
    "        print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e+1, np.mean(scores_window)))\n",
    "    if counter > 100:\n",
    "        break\n",
    "        \n",
    "    # update progress widget bar\n",
    "    #timer.update(e+1)\n",
    "    \n",
    "#timer.finish()\n",
    "\n",
    "print('Average Score: {:.2f}'.format(np.mean(scores_window)))\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Elapsed time: {}\".format(timedelta(seconds=elapsed)))\n",
    "print(\"Saving checkpoint!\")\n",
    "# save your policy!\n",
    "torch.save(policy.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
